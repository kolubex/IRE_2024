{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10, 0, 0, 1, 11, 23, 34, 44, 192, 228, 18383]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def merge_sort(lst):\n",
    "    if len(lst) <= 1:\n",
    "        return lst\n",
    "    mid_index = len(lst) // 2\n",
    "    left_half = lst[:mid_index]\n",
    "    right_half = lst[mid_index:]\n",
    "    left_half = merge_sort(left_half)\n",
    "    right_half = merge_sort(right_half)\n",
    "    return merge(left_half, right_half)\n",
    "\n",
    "def merge(left, right):\n",
    "    merged = []  \n",
    "    left_pointer, right_pointer = 0, 0\n",
    "    while left_pointer < len(left) or right_pointer < len(right):\n",
    "        if left_pointer == len(left):\n",
    "            merged.extend(right[right_pointer:])\n",
    "            break\n",
    "        elif right_pointer == len(right):\n",
    "            merged.extend(left[left_pointer:])\n",
    "            break\n",
    "        elif left[left_pointer] <= right[right_pointer]:\n",
    "            merged.append(left[left_pointer])\n",
    "            left_pointer += 1\n",
    "        else:\n",
    "            merged.append(right[right_pointer])\n",
    "            right_pointer += 1\n",
    "    return merged\n",
    "arr = [23, 0, 34, -10, 44, 192, 0, 228, 18383, 1, 11]\n",
    "print(merge_sort(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"IRE_fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from groq import Groq\n",
    "import re\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* Running on public URL: https://9e2e5f017d5163c39a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9e2e5f017d5163c39a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "class CodeGen:\n",
    "    def __init__(self, vector_db_path, sent_to_code_path, model):\n",
    "        self.global_calls = 0\n",
    "        self.api_key = \"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\"\n",
    "        self.model = \"llama3-70b-8192\"\n",
    "        self.sent_to_code_path = sent_to_code_path\n",
    "        self.vector_db = torch.load(vector_db_path)\n",
    "        self.sent_to_code = self.get_sent_to_code()\n",
    "        # Initialize client for chat model (replace with your actual client setup)\n",
    "        self.client = Groq(api_key=self.api_key)\n",
    "    def clean_code(self, code, ref_codes):\n",
    "        if \"python\" in code:\n",
    "            # replace python with \"\"\n",
    "            code = code.replace(\"python\", \"\")\n",
    "            code = code.split(\"```\")[1]\n",
    "        for item in ref_codes:\n",
    "            code = code.replace(item, \"\")\n",
    "        # same string is there twice, remove it\n",
    "        s = code\n",
    "        code = s[:len(s)//2] if len(s) % 2 == 0 and s[:len(s)//2] == s[len(s)//2:] else s\n",
    "        return code\n",
    "    def get_sent_to_code(self):\n",
    "        filename = self.sent_to_code_path\n",
    "        data = json.load(open(filename))\n",
    "        sent_to_code = {}\n",
    "        for item in data:\n",
    "            code = item['golden_code']\n",
    "            sent = item['description']\n",
    "            sent_to_code[sent] = code\n",
    "        return sent_to_code\n",
    "    def retrieve_top_k(self, query, k):\n",
    "        rankings = {}\n",
    "        encoded_input = tokenizer(query, return_tensors='pt')\n",
    "        output = model(**encoded_input, output_hidden_states=True)\n",
    "        last_hidden_states = output.hidden_states[-1]\n",
    "        query_vec = last_hidden_states.mean(dim=1).detach().numpy()\n",
    "        for sent in self.vector_db:\n",
    "            vector = self.vector_db[sent]\n",
    "            score = torch.cosine_similarity(torch.tensor(query_vec), torch.tensor(vector), dim=1).item()\n",
    "            rankings[sent] = score\n",
    "        return sorted(rankings.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    def generate_text(self, prompt):\n",
    "        chat_completion = self.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            model=self.model,\n",
    "            temperature=0.0,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        self.global_calls += 1\n",
    "        return chat_completion.choices[0].message.content\n",
    "\n",
    "    def generate_plan(self, query):\n",
    "        prompt = f\"\"\"\n",
    "        Given this query: {query}, generate a plan which outlines the steps to solve the problem. This plan should be in such a\n",
    "        format that generating code for each step of the plan should generate the final code for the query. Output the plan with each\n",
    "        step separated by a \"||\". Output only the plan. Do not output any additional information or additional text. In the plan, do not\n",
    "        include any steps to test the generated code. Your plan should only include the steps to generate the code, modularly.\n",
    "        \"\"\"\n",
    "        return self.generate_text(prompt)\n",
    "\n",
    "    def generate_code_with_planning(self, query):\n",
    "        plan = self.generate_plan(query)\n",
    "        steps = plan.split(\"||\")\n",
    "        code = \"\"\n",
    "        steps_cnt = 1\n",
    "        for step in steps:\n",
    "            prompt = f\"\"\"\n",
    "            You are generating code based on a plan. Currently, you have to generate code for the following step: {step}.\n",
    "            Until now, this code has been generated: {code}. Use the code generated until now to generate the code for the current step.\n",
    "            Output the final code based on the previous code and by generating code for the current step.\n",
    "            Output the final code up to the current step only. Do not output any additional information or additional text.\n",
    "            \"\"\"\n",
    "            code = self.generate_text(prompt)\n",
    "            # Yield the current plan and generated code\n",
    "            yield f\"Step: {steps_cnt}, Plan: {step}\\nGenerated Code:\\n{code}\\n{'-' * 100}\"\n",
    "            time.sleep(1)\n",
    "            steps_cnt += 1\n",
    "        yield f\"Final Code:\\n{self.clean_code(code, [])}\"\n",
    "    def generate_code_retrieve(self, query, k):\n",
    "        top_k = self.retrieve_top_k(query, k)\n",
    "        codes = [self.sent_to_code[sent] for sent, _ in top_k]\n",
    "        if len(codes) == 0:\n",
    "            prompt = f\"\"\"\n",
    "            Generate code for the following query: {query}.\n",
    "            Only output the generated code. Do not output any additional information or additional text.\n",
    "            Output only the required code. Do not generate any additional code, text or example usage information.\n",
    "            \"\"\"\n",
    "        else:\n",
    "            print(\"Retrieved Codes: \", codes)\n",
    "            prompt = f\"\"\"\n",
    "            Generate code for the following query: {query}.\n",
    "            Make use of these codes as reference while generating the code {codes}. Use the above codes only as reference. Do not copy the code as it is.\n",
    "            Only output the generated code. Do not output any additional information or additional text.\n",
    "            Output only the required code. Do not generate any additional code, text or example usage information.\n",
    "            \"\"\"\n",
    "        gen_code = self.generate_text(prompt)\n",
    "        cleaned_code = self.clean_code(gen_code, codes)\n",
    "        return cleaned_code\n",
    "    def generate_code_with_planning_retrieval(self, query, k):\n",
    "        plan = self.generate_plan(query)\n",
    "        ref_codes = self.retrieve_top_k(query, k)\n",
    "        steps = plan.split(\"||\")\n",
    "        code = \"\"\n",
    "        steps_cnt = 1\n",
    "        print(\"Retrieved Codes: \", ref_codes)\n",
    "        for step in steps:\n",
    "            # Simulated retrieval for this example  # Replace with actual retrieval logic\n",
    "            prompt = f\"\"\"\n",
    "            You are generating code based on a plan. Currently, you have to generate code for the following step: {step}.\n",
    "            Until now, this code has been generated: {code}. Use the code generated until now to generate the code for the current step.\n",
    "            You may use this code as reference: {ref_codes}.\n",
    "            Output the final code based on the previous code and by generating code for the current step.\n",
    "            Output the final code up to the current step only. Do not output any additional information or additional text.\n",
    "            \"\"\"\n",
    "            code = self.generate_text(prompt)\n",
    "            # Yield the current plan and generated code\n",
    "            # sleep for 1 sec\n",
    "            time.sleep(1)\n",
    "            yield f\"Step: {steps_cnt}, Plan: {step}\\nGenerated Code:\\n{code}\\n{'-' * 100}\"\n",
    "            steps_cnt += 1\n",
    "        yield f\"Final Code:\\n{self.clean_code(code, [])}\"\n",
    "\n",
    "# Initialize CodeGen instance\n",
    "code_generator = CodeGen(\"data/vector_db.pt\", \"data/gold_codes_difficult.json\", model=\"llama3-7b\")\n",
    "\n",
    "# Gradio interface\n",
    "def generate_code(user_input, retrieval, planning):\n",
    "    if planning:\n",
    "        if retrieval:\n",
    "            stream = code_generator.generate_code_with_planning_retrieval(user_input, 1)\n",
    "        else:\n",
    "            stream = code_generator.generate_code_with_planning(user_input)\n",
    "        for update in stream:\n",
    "            yield update, None  # Update \"Planning Text\" box\n",
    "    else:\n",
    "        if retrieval:\n",
    "            code = code_generator.generate_code_retrieve(user_input, 1)\n",
    "        else:\n",
    "            code = code_generator.generate_code_retrieve(user_input, 0)\n",
    "        yield None, code  # Update \"Generated Output Code\" box\n",
    "def generate_code_real_time(user_input, retrieval, planning):\n",
    "    if planning:\n",
    "        if retrieval:\n",
    "            stream = code_generator.generate_code_with_planning_retrieval(user_input, 1)\n",
    "        else:\n",
    "            stream = code_generator.generate_code_with_planning(user_input)\n",
    "        # Iterate through the updates for real-time display\n",
    "        for update in stream:\n",
    "            yield update, None  # Update \"Planning Text\" box\n",
    "    else:\n",
    "        if retrieval:\n",
    "            code = code_generator.generate_code_retrieve(user_input, 1)\n",
    "        else:\n",
    "            code = code_generator.generate_code_retrieve(user_input, 0)\n",
    "        yield None, code  # Update \"Generated Output Code\" box\n",
    "with gr.Blocks() as demo:\n",
    "    # Title and description\n",
    "    gr.Markdown(\"<h1 style='text-align: center; background-color: #34b9e8; color: white'>Code Generation - Team String</h1>\")\n",
    "    gr.Markdown(\"An interface to generate code based on user input. Allows for various parameters to be set to customize the generation.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        retrieval = gr.Checkbox(label=\"Enable Retrieval\", value=False)\n",
    "        planning = gr.Checkbox(label=\"Utilize Planning\", value=False)\n",
    "\n",
    "    with gr.Row():\n",
    "        input_text = gr.Textbox(label=\"Enter your prompt\", lines=5)\n",
    "        output_text_1 = gr.Code(label=\"Live Code Updates\", language=\"python\", lines=10)\n",
    "        output_text_2 = gr.Code(label=\"Generated Output Code\", language=\"python\", lines=8)\n",
    "\n",
    "    submit_btn = gr.Button(\"Generate\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "    # Link function to Gradio components\n",
    "    submit_btn.click(\n",
    "    fn=generate_code_real_time,\n",
    "    inputs=[input_text, retrieval, planning],\n",
    "    outputs=[output_text_1, output_text_2]\n",
    ")\n",
    "# Launch the app\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
